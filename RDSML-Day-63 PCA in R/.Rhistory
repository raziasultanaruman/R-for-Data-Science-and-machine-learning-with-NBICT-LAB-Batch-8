# updating the file
data = c(9, 6, 17, 31, 11)
x =  mean(data)
mean(data)
summary(data)
z = x/2
data = c(9, 6, 17, 31, 11)
mean(data)
summary(data)
# The Plyr package is used to calculate the mean weight of each group.
install.packages("Plyr")
# The Plyr package is used to calculate the mean weight of each group.
install.packages("Plyr")
# The Plyr package is used to calculate the mean weight of each group.
install.packages("Plyr")
# The Plyr package is used to calculate the mean weight of each group.
install.packages('Plyr')
# The Plyr package is used to calculate the mean weight of each group.
install.packages("Plyr")
# The Plyr package is used to calculate the mean weight of each group.
install.packages("Plyr")
# The Plyr package is used to calculate the mean weight of each group.
install.packages("Plyr")
ggsave("publication_ggplot.png", p, width = 6, height = 4, dpi = 300)
## Simple Linear Regression
# Importing the dataset
dataset <- read.csv("Salary_Data.csv")
# Importing the dataset
dataset <- read.csv("Salary_Data.csv")
library(readr)
Salary_Data <- read_csv("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-47  Simple Linear Regression in R (Part-2)/Salary_Data.csv")
View(Salary_Data)
# Importing the dataset
dataset <- read.csv("Salary_Data.csv")
dataset <- read.csv("Salary_Data.csv")
dataset <- read.csv("Salary_Data.csv")
# Importing the datase
dataset <- read.csv("Salary_Data")
# Importing the datase
dataset <- read.csv("Salary_Data.csv")
# Importing the datase
dataset <- read.csv("Salary_Data.csv")
install.packages("caTools")
library(caTools)
set.seed(123)
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-47  Simple Linear Regression in R (Part-2)")
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-47  Simple Linear Regression in R (Part-2)")
# Importing the datase
dataset <- read.csv("Salary_Data.csv")
## Simple Linear Regression
# Importing the datase
dataset <- read.csv("Salary_Data.csv")
# Splitting the dataset into the training set and test set
install.packages("caTools")
library(caTools)
set.seed(123)
split <- sample.split(dataset$Salary, SplitRatio = 2/3)
split
training_set <- subset(dataset, split == TRUE)
test_set <- subset(dataset, split == FALSE)
# Fitting the Simple Linear Regression model to the training set
regressor <- lm(formula = Salary ~ YearsExperience,
data = training_set)
# Checking the summary of the model
summary(regressor)
# Predicting the test set results
y_pred = predict(regressor, newdata = training_set)
# Visualizing the Training set results
library(ggplot2)
ggplot() +
geom_point(aes(x=training_set$YearsExperience, y=training_set$Salary),
colour='red') +
geom_line(aes(x=training_set$YearsExperience, y=y_pred),
colour='blue') +
ggtitle("Salary vs Experience") +
xlab('Years of experience') +
ylab('Salary')
# Visualizing the Test set results
library(ggplot2)
ggplot() +
geom_point(aes(x=test_set$YearsExperience, y=test_set$Salary),
colour='red') +
geom_line(aes(x=test_set$YearsExperience, y=predict(regressor, newdata = test_set)),
colour='blue') +
ggtitle("Salary vs Experience (Test Set)") +
xlab('Years of experience') +
ylab('Salary')
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-48  Multiple Linear Regression in R (Part-1)")
# Multiple Linear Regression
# Importing the dataset
dataset <- read.csv("50_Startups.csv")
# Encoding the categorical data
dataset$State <- factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1, 2, 3))
# Splitting the dataset into the Training Set and Test Set
library(caTools)
set.seed(123)
split <- sample.split(dataset$Profit, SplitRatio = 0.8)
training_set <- subset(dataset, split == TRUE)
test_set <- subset(dataset, split == FALSE)
# Fitting the Multiple Linear Regression Model to the Training Set
regressor <- lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data = training_set)
summary(regressor)
# Predicting the test set results
y_pred = predict(regressor, test_set)
y_pred
# Backward Elimination
regressor1 <- lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data = training_set)
summary(regressor1)
regressor2 <- lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend,
data = training_set)
summary(regressor2)
regressor3 <- lm(formula = Profit ~ R.D.Spend + Marketing.Spend,
data = training_set)
summary(regressor3)
# Full model
full_model <- lm(Profit ~ ., data = dataset)
# Automated Stepwise backward elimination
final_model <- step(full_model, direction = "backward")
summary(final_model)
# Checking the linearity assumption
plot(final_model)
dwtest(final_model)
# Importing the dataset
dataset <- read.csv("50_Startups.csv")
dataset$State <- factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1, 2, 3))
# Importing the dataset
dataset <- read.csv("50_Startups.csv")
dataset$State <- factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1, 2, 3))
library(caTools)
set.seed(123)
split <- sample.split(dataset$Profit, SplitRatio = 0.8)
training_set <- subset(dataset, split == TRUE)
test_set <- subset(dataset, split == FALSE)
regressor <- lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data = training_set)
summary(regressor)
y_pred = predict(regressor, test_set)
y_pred
regressor1 <- lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data = training_set)
summary(regressor1)
regressor1 <- lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data = training_set)
summary(regressor1)
regressor2 <- lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend,
data = training_set)
summary(regressor2)
regressor3 <- lm(formula = Profit ~ R.D.Spend + Marketing.Spend,
data = training_set)
summary(regressor3)
# Full model
full_model <- lm(Profit ~ ., data = dataset)
final_model <- step(full_model, direction = "backward")
summary(final_model)
# Checking the linearity assumption
plot(final_model)
dwtest(final_model)
# checking for the independence of errors assumption
install.packages("lmtest")
library(lmtest)
dwtest(final_model)
plot(final_model$fitted.values, rstandard(final_model))
abline(h = 0, col = "red")
bptest(final_model)
qqnorm(rstandard(final_model))
qqline(rstandard(final_model))
shapiro.test(rstandard(final_model))
library(car)
vif(final_model)
# Importing the dataset
dataset <- read.csv("50_Startups.csv")
View(dataset)
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-49  Multiple Linear Regression in R (Part-2)")
# Importing the dataset
dataset <- read.csv("50_Startups.csv")
View(dataset)
# Multiple Linear Regression
# Importing the dataset
dataset <- read.csv("50_Startups.csv")
# Encoding the categorical data
dataset$State <- factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1, 2, 3))
# Splitting the dataset into the Training Set and Test Set
library(caTools)
set.seed(123)
split <- sample.split(dataset$Profit, SplitRatio = 0.8)
training_set <- subset(dataset, split == TRUE)
test_set <- subset(dataset, split == FALSE)
# Fitting the Multiple Linear Regression Model to the Training Set
regressor <- lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend + State,
data = training_set)
summary(regressor)
# Predicting the test set results
y_pred <- predict(regressor, test_set)
y_pred
# Stepwise backward elimination
regressor <- lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend,
data = training_set)
summary(regressor)
regressor <- lm(formula = Profit ~ R.D.Spend + Marketing.Spend,
data = training_set)
summary(regressor)
# Automated Stepwise Backward Elimination
full_model <- lm(Profit ~ ., data = training_set)
summary(full_model)
final_model <- step(full_model, direction = "backward")
summary(final_model)
cor(training_set$R.D.Spend, training_set$Profit, method = 'pearson')
cor.test(training_set$R.D.Spend, training_set$Profit, method = 'pearson')
cor.test(training_set$Marketing.Spend, training_set$Profit, method = 'pearson')
plot(training_set$R.D.Spend, training_set$Profit)
plot(final_model)
dwtest(final_model)
# Normality of residuals
shapiro.test(rstandard(final_model))
qqnorm(rstandard(final_model))
qqline(rstandard(final_model))
# Checking multicollinearity
library(car)
vif(final_model)
# Homoscedasticity assumption
plot(final_model$fitted.values, rstandard(final_model))
abline(h=0, col="red")
bptest(final_model)
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-52 Polynomial Regression in R")
# Importing the dataset
dataset <- read.csv("polynom_data.csv")
attach(dataset)
# Plot the data
plot(x, y, main = "Polynomial Regression Example", pch = 19)
# Fit a quadratic (degree 2) polynomial regression
polynom_model <- lm(y ~ poly(x, 2, raw = TRUE))
# View the model summary
summary(polynom_model)
# Create new data for prediction
x_new <- seq(min(x), max(x), length.out = 100)
# x_new <- data.frame(x_new)
y_pred <- predict(polynom_model, newdata = data.frame(x = x_new))
pred_data <- data.frame(x_new, y_pred)
# Add fitted curve to the plot
lines(x_new, y_pred, col = "blue", lwd = 2)
# Fit a cubic (degree 3) polynomial regression
polynom_model3 <- lm(y ~ poly(x, 3, raw = TRUE))
summary(polynom_model3)
lines(x_new,
predict(polynom_model3, newdata = data.frame(x = x_new)),
col = "red", lwd = 2, lty = 2)
legend("topright", legend = c("Degree2", "Degree3"),
col = c("blue", "red"),
lty = c(1, 2),
lwd = 2)
View(dataset)
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-53 Support Vectors in R (Part-01)")
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-53 Support Vectors in R (Part-01)")
# Setup: packages and data loading
# install.packages(c("e1071","ggplot2","dplyr"))
library(e1071)
library(ggplot2)
library(dplyr)
# Loading the dataset
dataset <- read.csv("day.csv")
# Basic cleaning and feature selection
bike <- dataset %>%
select(cnt, temp, atemp, hum, windspeed, season, yr, mnth,
holiday, weekday, workingday, weathersit)
str(bike)
# Convert categorical variables to factors
bike <- bike %>%
mutate(
season = factor(season),
yr = factor(yr),
mnth = factor(mnth),
holiday = factor(holiday),
weekday = factor(weekday),
workingday = factor(workingday),
weathersit = factor(weathersit)
)
str(bike)
# Train/Test Split
set.seed(123)
n <- nrow(bike)
indices <- seq_len(n)
train_idx <- sample(indices, size = 0.7 * n)
train_set <- bike[train_idx, ]
test_set <- bike[-train_idx, ]
View(dataset)
View(train_set)
View(test_set)
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-55 Support Vector Regression in R (Part-3)")
# Setup: packages and data loading
# install.packages(c("e1071","ggplot2","dplyr"))
library(e1071)
library(ggplot2)
library(dplyr)
# Loading the dataset
dataset <- read.csv("day.csv")
# Basic cleaning and feature selection
bike <- dataset %>%
select(cnt, temp, atemp, hum, windspeed, season, yr, mnth,
holiday, weekday, workingday, weathersit)
str(bike)
# Convert categorical variables to factors
bike <- bike %>%
mutate(
season = factor(season),
yr = factor(yr),
mnth = factor(mnth),
holiday = factor(holiday),
weekday = factor(weekday),
workingday = factor(workingday),
weathersit = factor(weathersit)
)
str(bike)
# Train/Test Split
set.seed(123)
n <- nrow(bike)
indices <- seq_len(n)
train_idx <- sample(indices, size = 0.7 * n)
train_set <- bike[train_idx, ]
test_set <- bike[-train_idx, ]
# Scale numeric features
# Identify numeric predictors
num_cols <- c("temp", "atemp", "hum", "windspeed")
# Compute scaling parameters on training data
train_means <- sapply(train_set[, num_cols], mean)
train_sds <- sapply(train_set[, num_cols], sd)
# Scale Function
scale_num <- function(df){
df[, num_cols] <- sweep(df[, num_cols], 2, train_means, "-")
df[, num_cols] <- sweep(df[, num_cols], 2, train_sds, "/")
df
}
train_scaled <- scale_num(train_set)
test_scaled <- scale_num(test_set)
# Building a linear regression model
lm_model <- lm(cnt ~ ., data = train_scaled)
summary(lm_model)
lm_pred <- predict(lm_model, newdata = test_scaled)
# Evaluation metrics for linear regression
# Calculating root mean squared error
rmse <- function(actual, pred) sqrt(mean((actual-pred)^2))
lm_rmse <- rmse(test_scaled$cnt, lm_pred)
lm_rmse
# Calculating mean absolute error
mae <- function(actual, pred) mean(abs(actual-pred))
lm_mae <- mae(test_scaled$cnt, lm_pred)
lm_mae
# Building svr model
svr <- svm(cnt ~ ., data = train_scaled, kernel = "linear")
svr_pred <- predict(svr, newdata = test_scaled)
svr_rmse <- rmse(test_scaled$cnt, svr_pred)
svr_mae <- mae(test_scaled$cnt, svr_pred)
# Train/Test Split
set.seed(123)
n <- nrow(bike)
indices <- seq_len(n)
train_idx <- sample(indices, size = 0.7 * n)
train_set <- bike[train_idx, ]
test_set <- bike[-train_idx, ]
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-59 Decision Tree Regression in R")
# Decision Tree Regression
# Importing the dataset
dataset = read.csv('Drug_Effectiveness.csv')
# Plotting the data
plot(dataset$Dosage, dataset$Effectiveness,
main = "Scatter Plot of Dosage vs. Drug Effectiveness",
xlab = "Dosage",
ylab = "Drug Effectiveness")
# Fitting Decision Tree Regression to the dataset
# install.packages('rpart')
library(rpart)
regressor = rpart(formula = Effectiveness ~ .,
data = dataset,
control = rpart.control(minsplit = 7))
# Predicting a new result with Decision Tree Regression
y_pred = predict(regressor, data.frame(Dosage = 27))
y_pred
# Visualising the Decision Tree Regression results (higher resolution)
# install.packages('ggplot2')
library(ggplot2)
x_grid = seq(min(dataset$Dosage), max(dataset$Dosage), 0.01)
ggplot() +
geom_point(aes(x = dataset$Dosage, y = dataset$Effectiveness),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Dosage = x_grid))),
colour = 'blue') +
ggtitle('Decision Tree Regression') +
xlab('Dosage') +
ylab('Effectiveness')
# install.packages("rpart.plot")
library(rpart.plot)
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(regressor)
rpart.plot(regressor, type = 2, fallen.leaves = TRUE, tweak = 1.2)
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-60 Random Forest Regression in R")
# Random Forest Regression
# Importing the dataset
dataset = read.csv('Drug_Effectiveness.csv')
library(randomForest)
install.packages(randomForest)
install.packages("randomForest")
library(randomForest)
set.seed(1234)
regressor = randomForest(x = dataset[1],
y = dataset$Effectiveness,
ntree = 10)
y_pred = predict(regressor, data.frame(Dosage = 20))
y_pred
# Visualising the Random Forest Regression results (higher resolution)
# install.packages('ggplot2')
library(ggplot2)
x_grid = seq(min(dataset$Dosage), max(dataset$Dosage), 0.01)
ggplot() +
geom_point(aes(x = dataset$Dosage, y = dataset$Effectiveness),
colour = 'red') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Dosage = x_grid))),
colour = 'blue') +
ggtitle('Dosage VS Drug Effectivenes') +
xlab('Dosage') +
ylab('Drug Effectivenes')
# Importing the dataset
dataset = read.csv('Drug_Effectiveness.csv')
View(dataset)
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-61 logistic Regression in R")
# Logistic Regression
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting Logistic Regression to the Training set
classifier = glm(formula = Purchased ~ .,
family = binomial,
data = training_set)
# Predicting the Test set results
prob_pred = predict(classifier, type = 'response', newdata = test_set[-3])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-62 K-fold Cross Validation")
# Random Forest Classification
# Importing the dataset
dataset <- read.csv('Social_Network_Ads.csv')
dataset <- dataset[3:5]
# Encoding the target variable as factor
dataset$Purchased <- factor(dataset$Purchased,
levels = c(0, 1))
# Implementing k-fold cross validation
# install.packages('caret')
library(caret)
# Implementing k-fold cross validation
install.packages('caret')
library(caret)
library(caret)
library(caret)
# Implementing k-fold cross validation
install.packages('caret')
library(caret)
# Implementing k-fold cross validation
install.packages('caret')
# Implementing k-fold cross validation
install.packages("caret")
# Implementing k-fold cross validation
install.packages("caret")
set.seed(123)
folds = createFolds(dataset$Purchased, k = 10)
set.seed(123)
folds = createFolds(dataset$Purchased, k = 10)
install.packages("caret")
setwd("D:/R/R-for-Data-Science-and-Machine-Learning-with-NBICT-Lab-Batch-8/RDSML-Day-63 PCA in R")
# PCA
# Importing the dataset
dataset = read.csv('Wine.csv')
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-14] = scale(training_set[-14])
test_set[-14] = scale(test_set[-14])
# Applying PCA
# install.packages('caret')
library(caret)
# Splitting the dataset into the Training set and Test set
install.packages("caTools")
library(caTools)
# Applying PCA
install.packages("caret")
library(caret)
